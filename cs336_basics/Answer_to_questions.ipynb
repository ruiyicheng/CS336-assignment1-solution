{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What Unicode character does chr(0) return?\n",
    "\n",
    "Deliverable: An empty string which also marks the end of a string\n",
    "(b) How does this character’s string representation (__repr__()) differ from its printed representation?\n",
    "Deliverable: A one-sentence response.\n",
    "(c) What happens when this character occurs in text? It may be helpful to play around with the\n",
    "following in your Python interpreter and see if it matches your expectations:\n",
    ">>> chr(0)\n",
    ">>> print(chr(0))\n",
    ">>> \"this is a test\" + chr(0) + \"string\"\n",
    ">>> print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)\n",
    "print(chr(0))\n",
    "# str(chr(0))\n",
    "chr(0).__repr__()\n",
    "chr(0).__str__()\n",
    "# print(str('This is a test '+chr(0)+\" string\"))\n",
    "# print(\"This is a test \"+chr(0)+\" string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd, Hello'\n",
      "<class 'bytes'>\n",
      "[228, 189, 160, 229, 165, 189, 44, 32, 72, 101, 108, 108, 111]\n",
      "13\n",
      "9\n",
      "你好, Hello\n"
     ]
    }
   ],
   "source": [
    "test_str = \"你好, Hello\"\n",
    "utf8_bytes = test_str.encode('utf-8')\n",
    "print(utf8_bytes)\n",
    "print(type(utf8_bytes))\n",
    "list_of_bytes = list(utf8_bytes)\n",
    "print(list_of_bytes)\n",
    "print(len(list_of_bytes))\n",
    "print(len(test_str))\n",
    "print(utf8_bytes.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfe\\x00\\x00`O\\x00\\x00}Y\\x00\\x00,\\x00\\x00\\x00 \\x00\\x00\\x00H\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00'\n",
      "[255, 254, 0, 0, 96, 79, 0, 0, 125, 89, 0, 0, 44, 0, 0, 0, 32, 0, 0, 0, 72, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0]\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "utf32_bytes = test_str.encode('utf-32')\n",
    "print(utf32_bytes)\n",
    "print(list(utf32_bytes))\n",
    "print(len(list(utf32_bytes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mbytes\u001b[39m([b])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n\u001b[1;32m      3\u001b[0m byte_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m你好\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mdecode_utf8_bytes_to_str_wrong\u001b[0;34m(bytestring)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mbytes\u001b[39m([b])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "byte_ = '你好'.encode('utf-8')\n",
    "decode_utf8_bytes_to_str_wrong(byte_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-ASCII code (more than 1 byte) should not be decoded in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m seq \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0b11000000\u001b[39m,\u001b[38;5;241m0b11000000\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# convert seq to bytes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "seq = [0b11000000,0b11000000]\n",
    "# convert seq to bytes\n",
    "bytes(seq).decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reason for failure: For two byte chraracter in UTF-8, it must be 0b110XXXXX 0b10XXXXXX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_str=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(pat_str, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable parameters in GPT-2 XL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding : vocab_size $\\times$ d_model = 50,257 $\\times$ 1,600 = 80411200\n",
    "- 48 transformer block: \n",
    "    - 2 RMS norm: 2 $\\times$ d_model = 2 $\\times$ 1,600 = 3200\n",
    "    - 1 multiheadselfattention\n",
    "        - 4 embedding matrix WQ WK WV WO: 4 $\\times$ d_model $\\times$ d_model = 4 $\\times$ 1600 $\\times$ 1600 = 10240000\n",
    "    - FFN: SwiGLU: 3 $\\times$ d_ff $\\times$ d_model = 3 $\\times$ 6400 $\\times$ 1600 = 30720000\n",
    "48 $\\times$ (30720000+10240000+3200) = 1966233600\n",
    "- final layers: 1600 + 80411200 = 80412800\n",
    "\n",
    "Total:\n",
    "80411200 + 1966233600 + 80412800 = 2,127,057,600 learnable parameters\n",
    "\n",
    "when using FP32, it requires 2,127,057,600 $\\times$ 32/8 / $2^30$ = 7.92 GB memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix multiplies and FLOPS of GPT-2 XL\n",
    "- Embedding: No matrix multiplication\n",
    "- 48 transformer block:\n",
    "    - Q K V projection:3 $\\times$ 2 seq_length $\\times$ d_model $\\times$ seq_length\n",
    "    - RoPE for Q K : 4 $\\times$ seq_length $\\times$ d_model\n",
    "    - attention : 4 seq_length $\\times$ d_model $\\times$ seq_length\n",
    "    - FFN : 6 $\\times$ d_ff $\\times$ d_model $\\times$ seq_length\n",
    "- Final linear projection: 2 seq_length $\\times$ d_model $\\times$ vocab_size\n",
    "\n",
    "Total leading order part:\n",
    "\n",
    "48 ($\\times$ 10 d_model $\\times$ (seq_length)$^2$ + 6 $\\times$ d_ff $\\times$ d_model $\\times$ seq_length) + 2 seq_length $\\times$ d_model $\\times$ vocab_size = 48 $\\times$ (16777216000 + 62914560000) + 164682137600 = 3989887385600 FLOPs= 3.715 TFLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small: 325.61865234375 GFLOPs\n",
      "Med: 642.158203125 GFLOPs\n",
      "large: 1922.69775390625 GFLOPs\n",
      "XL: 3715.8721923828125 GFLOPs\n",
      "XL-context length to 16384: 239453.955078125 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "def Gflops(layer,d_model,seq_length = 1024,vocab_size = 50257):\n",
    "    return (layer * (10 * d_model * seq_length**2 + 6 * 4 * d_model**2 * seq_length) + 2* seq_length * d_model * vocab_size)/2**30\n",
    "print(\"Small:\",Gflops(12,768),'GFLOPs')\n",
    "print(\"Med:\",Gflops(16,1024),'GFLOPs')\n",
    "print(\"large:\",Gflops(36,1280),'GFLOPs')\n",
    "print(\"XL:\",Gflops(48,1600),'GFLOPs')\n",
    "print(\"XL-context length to 16384:\",Gflops(48,1600,seq_length = 16384),'GFLOPs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource counting for training using AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab_size, context_length,\n",
    "num_layers, d_model, num_heads, d_ff = 4 d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.01252269744873\n",
      "80.16926097869873\n"
     ]
    }
   ],
   "source": [
    "def memory_consumption_Adam(vocab_size = 50257, context_length  = 1024,num_layers = 48, d_model = 1600, num_heads = 25, batch_size = 1):\n",
    "    d_ff = 4 * d_model\n",
    "    total_memory_consumption_float = 0\n",
    "    # weights:\n",
    "    # embedding\n",
    "    embedding_memory = vocab_size * d_model\n",
    "    # transformer\n",
    "    RMS_norm_memory = 2 * d_model\n",
    "    KQVO = 4 * d_model ** 2\n",
    "    FFN = 2 * d_model * d_ff\n",
    "    transformer_memory_per_layer = RMS_norm_memory + KQVO + FFN\n",
    "    transformer_memory_total = num_layers * transformer_memory_per_layer\n",
    "    # output\n",
    "    embedding_memory_out = vocab_size * d_model +d_model\n",
    "    total_LLM_weights_memory = embedding_memory + transformer_memory_total + embedding_memory_out\n",
    "\n",
    "    # Activations :\n",
    "    RMS_block = 2 * context_length * d_model\n",
    "    KQV =  context_length * d_model\n",
    "    KQ = context_length ** 2\n",
    "    softmax = context_length ** 2\n",
    "    K = context_length ** 2\n",
    "    Q = context_length ** 2\n",
    "\n",
    "    FFN = 3 * d_model * d_ff  +  context_length * d_model\n",
    "\n",
    "    transformers = (RMS_block+KQV+KQ+softmax+K+Q) * num_layers\n",
    "    Final_RMS = context_length * d_model\n",
    "    Final_embedding = context_length * d_model\n",
    "    activations = (transformers + Final_RMS + Final_embedding) * batch_size\n",
    "\n",
    "\n",
    "\n",
    "    # Gradients and AdamW status\n",
    "    gradients = total_LLM_weights_memory\n",
    "    Adamstats = 2 * total_LLM_weights_memory # m and v\n",
    "    optimizer_memory = gradients + Adamstats\n",
    "\n",
    "    # print(total_LLM_weights_memory/2**30)\n",
    "    # print(activations/2**30)\n",
    "    # print(optimizer_memory/2**30)\n",
    "    all_mem = (total_LLM_weights_memory + activations + optimizer_memory)/2**30\n",
    "    # print(all_mem)\n",
    "    return all_mem*4\n",
    "\n",
    "print(memory_consumption_Adam())\n",
    "print(memory_consumption_Adam(batch_size = 34))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For batch = 1, flops = 3.71 * 3 = 11.129 TFLOPS\n",
    "# For batch = 1024 ;flops = 11397 Flops\n",
    "# for 400 K round, 400000*11397/(19.5/2) = 467574153s = 5411 days"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
